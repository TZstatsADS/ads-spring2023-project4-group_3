{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "99cb0cb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99cb0cb0",
        "outputId": "9e7e154b-48d4-42df-d8dd-a74e6dbf3d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==57.5.0\n",
            "  Downloading setuptools-57.5.0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.1\n",
            "    Uninstalling setuptools-67.6.1:\n",
            "      Successfully uninstalled setuptools-67.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 57.5.0 which is incompatible.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 57.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-57.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dccp\n",
            "  Downloading dccp-1.0.4.tar.gz (8.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from dccp) (1.3.1)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.9/dist-packages (from cvxpy>=0.3.5->dccp) (2.0.12)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from cvxpy>=0.3.5->dccp) (0.6.2.post0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from cvxpy>=0.3.5->dccp) (1.10.1)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.9/dist-packages (from cvxpy>=0.3.5->dccp) (3.2.3)\n",
            "Collecting setuptools>65.5.1\n",
            "  Downloading setuptools-67.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/dist-packages (from cvxpy>=0.3.5->dccp) (1.22.4)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.9/dist-packages (from osqp>=0.4.1->cvxpy>=0.3.5->dccp) (0.1.7)\n",
            "Building wheels for collected packages: dccp\n",
            "  Building wheel for dccp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dccp: filename=dccp-1.0.4-py3-none-any.whl size=7388 sha256=f92bcafc554df328f1a8818a6446c7c0e6767286916ae08823663df9c69ca6af\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/44/67/21a6ef3024d3287644a46855ac39cdcd0aca4a8e34807efec0\n",
            "Successfully built dccp\n",
            "Installing collected packages: setuptools, dccp\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.5.0\n",
            "    Uninstalling setuptools-57.5.0:\n",
            "      Successfully uninstalled setuptools-57.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dccp-1.0.4 setuptools-67.6.1\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade pip\n",
        "!pip install setuptools==57.5.0 #adde\n",
        "!pip3 install dccp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import traceback\n",
        "import dccp\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import cvxpy\n",
        "import sys\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "90d236a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90d236a2",
        "outputId": "d97cdcad-be65-450f-d0c0-f9bc10fa04c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7214 entries, 0 to 7213\n",
            "Data columns (total 53 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       7214 non-null   int64  \n",
            " 1   name                     7214 non-null   object \n",
            " 2   first                    7214 non-null   object \n",
            " 3   last                     7214 non-null   object \n",
            " 4   compas_screening_date    7214 non-null   object \n",
            " 5   sex                      7214 non-null   object \n",
            " 6   dob                      7214 non-null   object \n",
            " 7   age                      7214 non-null   int64  \n",
            " 8   age_cat                  7214 non-null   object \n",
            " 9   race                     7214 non-null   object \n",
            " 10  juv_fel_count            7214 non-null   int64  \n",
            " 11  decile_score             7214 non-null   int64  \n",
            " 12  juv_misd_count           7214 non-null   int64  \n",
            " 13  juv_other_count          7214 non-null   int64  \n",
            " 14  priors_count             7214 non-null   int64  \n",
            " 15  days_b_screening_arrest  6907 non-null   float64\n",
            " 16  c_jail_in                6907 non-null   object \n",
            " 17  c_jail_out               6907 non-null   object \n",
            " 18  c_case_number            7192 non-null   object \n",
            " 19  c_offense_date           6055 non-null   object \n",
            " 20  c_arrest_date            1137 non-null   object \n",
            " 21  c_days_from_compas       7192 non-null   float64\n",
            " 22  c_charge_degree          7214 non-null   object \n",
            " 23  c_charge_desc            7185 non-null   object \n",
            " 24  is_recid                 7214 non-null   int64  \n",
            " 25  r_case_number            3471 non-null   object \n",
            " 26  r_charge_degree          3471 non-null   object \n",
            " 27  r_days_from_arrest       2316 non-null   float64\n",
            " 28  r_offense_date           3471 non-null   object \n",
            " 29  r_charge_desc            3413 non-null   object \n",
            " 30  r_jail_in                2316 non-null   object \n",
            " 31  r_jail_out               2316 non-null   object \n",
            " 32  violent_recid            0 non-null      float64\n",
            " 33  is_violent_recid         7214 non-null   int64  \n",
            " 34  vr_case_number           819 non-null    object \n",
            " 35  vr_charge_degree         819 non-null    object \n",
            " 36  vr_offense_date          819 non-null    object \n",
            " 37  vr_charge_desc           819 non-null    object \n",
            " 38  type_of_assessment       7214 non-null   object \n",
            " 39  decile_score.1           7214 non-null   int64  \n",
            " 40  score_text               7214 non-null   object \n",
            " 41  screening_date           7214 non-null   object \n",
            " 42  v_type_of_assessment     7214 non-null   object \n",
            " 43  v_decile_score           7214 non-null   int64  \n",
            " 44  v_score_text             7214 non-null   object \n",
            " 45  v_screening_date         7214 non-null   object \n",
            " 46  in_custody               6978 non-null   object \n",
            " 47  out_custody              6978 non-null   object \n",
            " 48  priors_count.1           7214 non-null   int64  \n",
            " 49  start                    7214 non-null   int64  \n",
            " 50  end                      7214 non-null   int64  \n",
            " 51  event                    7214 non-null   int64  \n",
            " 52  two_year_recid           7214 non-null   int64  \n",
            "dtypes: float64(4), int64(16), object(33)\n",
            "memory usage: 2.9+ MB\n"
          ]
        }
      ],
      "source": [
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "origin_df = pd.read_csv(url)\n",
        "origin_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "48338624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48338624",
        "outputId": "329a4511-c508-4a32-f25e-c45c4ba94716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6150 entries, 0 to 6149\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age_cat          6150 non-null   object\n",
            " 1   sex              6150 non-null   object\n",
            " 2   race             6150 non-null   object\n",
            " 3   priors_count     6150 non-null   int64 \n",
            " 4   c_charge_degree  6150 non-null   object\n",
            " 5   two_year_recid   6150 non-null   int64 \n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 288.4+ KB\n"
          ]
        }
      ],
      "source": [
        "df = origin_df.loc[:,[\"age_cat\",\"sex\",\"race\",\"priors_count\",\"c_charge_degree\",\"two_year_recid\"]].query('race in [\"African-American\",\"Caucasian\"]')\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "664ce231",
      "metadata": {
        "id": "664ce231"
      },
      "outputs": [],
      "source": [
        "features = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]\n",
        "cont_features = [\"priors_count\"]\n",
        "predicted_feature = \"two_year_recid\"\n",
        "sensitive_feature = \"race\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "827e9bc8",
      "metadata": {
        "id": "827e9bc8"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(df):\n",
        "    df = df.dropna(subset=[\"days_b_screening_arrest\"]).loc[:,features + [predicted_feature]].query('race in [\"African-American\",\"Caucasian\"]')\n",
        "\n",
        "    data = df.to_dict('list')\n",
        "    for k in data.keys():\n",
        "        data[k] = np.array(data[k])\n",
        "\n",
        "    y = data[predicted_feature]\n",
        "    y[y==0] = -1\n",
        "\n",
        "\n",
        "    X = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
        "    x_sensitive_feature = None\n",
        "\n",
        "    feature_names = []\n",
        "    for feature in features:\n",
        "        vals = data[feature]\n",
        "        if feature in cont_features:\n",
        "            vals = [float(v) for v in vals]\n",
        "            vals = preprocessing.scale(vals) \n",
        "            vals = np.reshape(vals, (len(y), -1)) \n",
        "\n",
        "        else: \n",
        "            lb = preprocessing.LabelBinarizer()\n",
        "            lb.fit(vals)\n",
        "            vals = lb.transform(vals)\n",
        "\n",
        "        if feature == sensitive_feature:\n",
        "            x_sensitive_feature = vals\n",
        "\n",
        "\n",
        "        X = np.hstack((X, vals))\n",
        "\n",
        "        if feature in cont_features: \n",
        "            feature_names.append(feature)\n",
        "        else: \n",
        "            if vals.shape[1] == 1: \n",
        "                feature_names.append(feature)\n",
        "            else:\n",
        "                for k in lb.classes_: \n",
        "                    feature_names.append(feature + \"_\" + str(k))\n",
        "\n",
        "    x_sensitive_feature = np.array(x_sensitive_feature).flatten()\n",
        "    X = np.concatenate((np.ones(X.shape[0]).reshape(X.shape[0], 1), X), axis = 1)\n",
        "\n",
        "    feature_names = [\"intercept\"] + feature_names\n",
        "    print(f\"Features we will be using for classification are: {feature_names}\")\n",
        "\n",
        "\n",
        "    return X, y, x_sensitive_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8c4f09a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c4f09a8",
        "outputId": "bcc1eb44-a2f3-456c-c722-c94a897354b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features we will be using for classification are: ['intercept', 'age_cat_25 - 45', 'age_cat_Greater than 45', 'age_cat_Less than 25', 'race', 'sex', 'priors_count', 'c_charge_degree']\n",
            "[[ 1.          1.          0.         ...  1.         -0.73366948\n",
            "   0.        ]\n",
            " [ 1.          0.          0.         ...  1.          0.05593295\n",
            "   0.        ]\n",
            " [ 1.          1.          0.         ...  1.          2.02993903\n",
            "   0.        ]\n",
            " ...\n",
            " [ 1.          0.          0.         ...  1.         -0.73366948\n",
            "   0.        ]\n",
            " [ 1.          0.          0.         ...  1.         -0.73366948\n",
            "   0.        ]\n",
            " [ 1.          1.          0.         ...  0.         -0.14146765\n",
            "   1.        ]] [ 1  1  1 ... -1 -1 -1] [0 0 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "X,y,x_race = data_preprocessing(origin_df)\n",
        "print(X,y,x_race)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c11a2631",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c11a2631",
        "outputId": "b2fe7e45-5092-4dc3-8198-0cac983f3098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5070, 8) (845, 8) (5915, 8)\n"
          ]
        }
      ],
      "source": [
        "X_train,X_test,y_train,y_test,x_race_train,x_race_test = train_test_split(X,y,x_race,test_size=1/7,random_state=5243)\n",
        "print(X_train.shape,X_test.shape,X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zMQgd6opkTS5",
      "metadata": {
        "id": "zMQgd6opkTS5"
      },
      "outputs": [],
      "source": [
        "def get_distance_boundary(w, x, s_attr_arr):\n",
        "    distances_boundary = np.zeros(x.shape[0])\n",
        "    if isinstance(w, dict): \n",
        "        for k in w.keys(): \n",
        "            d = np.dot(x, w[k])\n",
        "            distances_boundary[s_attr_arr == k] = d[s_attr_arr == k] \n",
        "    else: \n",
        "        distances_boundary = np.dot(x, w)\n",
        "    return distances_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "iLNzqzOalUKV",
      "metadata": {
        "id": "iLNzqzOalUKV"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_encoding(in_arr):\n",
        "    in_arr = np.array(in_arr, dtype=int)\n",
        "    attr_vals_uniq_sorted = sorted(list(set(in_arr)))\n",
        "    num_uniq_vals = len(attr_vals_uniq_sorted)\n",
        "    if (num_uniq_vals == 2) and (attr_vals_uniq_sorted[0] == 0 and attr_vals_uniq_sorted[1] == 1):\n",
        "        return in_arr, None\n",
        "\n",
        "    \n",
        "    index_dict = {}\n",
        "    for i in range(0,len(attr_vals_uniq_sorted)):\n",
        "        val = attr_vals_uniq_sorted[i]\n",
        "        index_dict[val] = i\n",
        "\n",
        "    out_arr = []    \n",
        "    for i in range(0,len(in_arr)):\n",
        "        tup = np.zeros(num_uniq_vals)\n",
        "        val = in_arr[i]\n",
        "        ind = index_dict[val]\n",
        "        tup[ind] = 1 # set that value of tuple to 1\n",
        "        out_arr.append(tup)\n",
        "\n",
        "    return np.array(out_arr), index_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3fJ0emvL7KrF",
      "metadata": {
        "id": "3fJ0emvL7KrF"
      },
      "outputs": [],
      "source": [
        "def get_constraint_list_cov(x_train, y_train, x_control_train, sensitive_attrs_to_cov_thresh, cons_type, w):\n",
        "\n",
        "    \"\"\"\n",
        "    get the list of constraints to be fed to the minimizer\n",
        "    cons_type == 0: means the whole combined misclassification constraint (without FNR or FPR)\n",
        "    cons_type == 1: FPR constraint\n",
        "    cons_type == 2: FNR constraint\n",
        "    cons_type == 4: both FPR as well as FNR constraints\n",
        "    sensitive_attrs_to_cov_thresh: is a dict like {s: {cov_type: val}}\n",
        "    s is the sensitive attr\n",
        "    cov_type is the covariance type. contains the covariance for all misclassifications, FPR and for FNR etc\n",
        "    \"\"\"\n",
        "\n",
        "    constraints = []\n",
        "\n",
        "    attr_arr = x_control_train\n",
        "    attr_arr_transformed, index_dict = get_one_hot_encoding(attr_arr)\n",
        "            \n",
        "    if index_dict is None: # binary attribute, in this case, the attr_arr_transformed is the same as the attr_arr\n",
        "\n",
        "        s_val_to_total = {ct:{} for ct in [0,1,2]} # constrain type -> sens_attr_val -> total number\n",
        "        s_val_to_avg = {ct:{} for ct in [0,1,2]}\n",
        "        cons_sum_dict = {ct:{} for ct in [0,1,2]} # sum of entities (females and males) in constraints are stored here\n",
        "\n",
        "        for v in set(attr_arr):\n",
        "            s_val_to_total[0][v] = sum(x_control_train == v)\n",
        "            s_val_to_total[1][v] = sum(np.logical_and(x_control_train == v, y_train == -1)) # FPR constraint so we only consider the ground truth negative dataset for computing the covariance\n",
        "            s_val_to_total[2][v] = sum(np.logical_and(x_control_train == v, y_train == +1))\n",
        "\n",
        "\n",
        "        for ct in [0,1,2]:\n",
        "            s_val_to_avg[ct][0] = s_val_to_total[ct][1] / float(s_val_to_total[ct][0] + s_val_to_total[ct][1]) # N1/N in our formulation, differs from one constraint type to another\n",
        "            s_val_to_avg[ct][1] = 1.0 - s_val_to_avg[ct][0] # N0/N\n",
        "\n",
        "        \n",
        "        for v in set(attr_arr):\n",
        "\n",
        "            idx = x_control_train == v                \n",
        "\n",
        "            dist_bound_prod = cvxpy.multiply(y_train[idx], x_train[idx] * w) # y.f(x)\n",
        "            \n",
        "            cons_sum_dict[0][v] = cvxpy.sum( cvxpy.minimum(0, dist_bound_prod) ) * (s_val_to_avg[0][v] / len(x_train)) # avg misclassification distance from boundary\n",
        "            cons_sum_dict[1][v] = cvxpy.sum( cvxpy.minimum(0, cvxpy.multiply( (1 - y_train[idx])/2.0, dist_bound_prod) ) ) * (s_val_to_avg[1][v] / sum(y_train == -1)) # avg false positive distance from boundary (only operates on the ground truth neg dataset)\n",
        "            cons_sum_dict[2][v] = cvxpy.sum( cvxpy.minimum(0, cvxpy.multiply( (1 + y_train[idx])/2.0, dist_bound_prod) ) ) * (s_val_to_avg[2][v] / sum(y_train == +1)) # avg false negative distance from boundary\n",
        "\n",
        "            \n",
        "        if cons_type == 4:\n",
        "            cts = [1,2]\n",
        "        elif cons_type in [0,1,2]:\n",
        "            cts = [cons_type]\n",
        "        \n",
        "        else:\n",
        "            raise Exception(\"Invalid constraint type\")\n",
        "\n",
        "\n",
        "        for ct in cts:\n",
        "            thresh = abs(sensitive_attrs_to_cov_thresh[ct][1] - sensitive_attrs_to_cov_thresh[ct][0])\n",
        "            constraints.append( cons_sum_dict[ct][1] <= cons_sum_dict[ct][0]  + thresh )\n",
        "            constraints.append( cons_sum_dict[ct][1] >= cons_sum_dict[ct][0]  - thresh )\n",
        "\n",
        "\n",
        "    return constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bea280cd",
      "metadata": {
        "id": "bea280cd"
      },
      "outputs": [],
      "source": [
        "def train_model(X,y,x_sensitive_feature,eps,constraint_params=None):\n",
        "    max_iters = 100 \n",
        "    max_iter_dccp = 50 \n",
        "\n",
        "    \n",
        "    num_points, num_features = X.shape\n",
        "    w = cvxpy.Variable(num_features)\n",
        "\n",
        "    np.random.seed(5243)\n",
        "    w.value = np.random.rand(X.shape[1])\n",
        "\n",
        "    loss = cvxpy.sum(cvxpy.logistic(cvxpy.multiply(-y, X*w) )  ) / num_points \n",
        "\n",
        "    if constraint_params is None: # just train a simple classifier, no fairness constraints\n",
        "        constraints = []\n",
        "    else:\n",
        "        constraints = get_constraint_list_cov(X, y, x_sensitive_feature, constraint_params[\"sensitive_attrs_to_cov_thresh\"], constraint_params[\"cons_type\"], w)\n",
        "    p = cvxpy.Problem(cvxpy.Minimize(loss), [])\n",
        "    p.solve()\n",
        "\n",
        "    prob = cvxpy.Problem(cvxpy.Minimize(loss), constraints)\n",
        "    try:\n",
        "        tau, mu = 0.005, 1.2 # default dccp parameters, need to be varied per dataset\n",
        "        if constraint_params is not None: # in case we passed these parameters as a part of dccp constraints\n",
        "            if constraint_params.get(\"tau\") is not None: tau = constraint_params[\"tau\"]\n",
        "            if constraint_params.get(\"mu\") is not None: mu = constraint_params[\"mu\"]\n",
        "\n",
        "        prob.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10,\n",
        "            solver=cvxpy.ECOS, verbose=False, \n",
        "            feastol=eps, abstol=eps, reltol=eps,feastol_inacc=eps, abstol_inacc=eps, reltol_inacc=eps,\n",
        "            max_iters=max_iters, max_iter=max_iter_dccp)\n",
        "\n",
        "        \n",
        "        assert(prob.status == \"Converged\" or prob.status == \"optimal\")\n",
        "\n",
        "    except:\n",
        "        traceback.print_exc()\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(1)\n",
        "    w = np.array(w.value).flatten() \n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "z9iRHN7w7pam",
      "metadata": {
        "id": "z9iRHN7w7pam"
      },
      "outputs": [],
      "source": [
        "def get_clf_stats(w, x_train, y_train, x_control_train, x_test, y_test, x_control_test):\n",
        "\n",
        "\n",
        "    # compute distance from boundary\n",
        "    distances_boundary_train = get_distance_boundary(w, x_train, x_control_train)\n",
        "    distances_boundary_test = get_distance_boundary(w, x_test, x_control_test)\n",
        "\n",
        "    # compute the class labels\n",
        "    all_class_labels_assigned_train = np.sign(distances_boundary_train)\n",
        "    all_class_labels_assigned_test = np.sign(distances_boundary_test)\n",
        "\n",
        "\n",
        "    train_score, test_score, correct_answers_train, correct_answers_test = check_accuracy(None, x_train, y_train, x_test, y_test, all_class_labels_assigned_train, all_class_labels_assigned_test)\n",
        "\n",
        "  \n",
        "        \n",
        "    print_stats = False \n",
        "    s_attr_to_fp_fn_train = get_fpr_fnr_sensitive_features(y_train, all_class_labels_assigned_train, x_control_train, print_stats)\n",
        "    cov_all_train = get_sensitive_attr_constraint_fpr_fnr_cov(None, x_train, y_train, distances_boundary_train, x_control_train) \n",
        "    \n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f\"Accuracy: {test_score}\")\n",
        "    print_stats = True # only print stats for the test fold\n",
        "    s_attr_to_fp_fn_test = get_fpr_fnr_sensitive_features(y_test, all_class_labels_assigned_test, x_control_test, print_stats)\n",
        "    cov_all_test = get_sensitive_attr_constraint_fpr_fnr_cov(None, x_test, y_test, distances_boundary_test, x_control_test) \n",
        "    print(\"\\n\")\n",
        "\n",
        "    return train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8t-hmYn271zU",
      "metadata": {
        "id": "8t-hmYn271zU"
      },
      "outputs": [],
      "source": [
        "def get_fpr_fnr_sensitive_features(y_true, y_pred, x_control, verbose = False):\n",
        "\n",
        "\n",
        "\n",
        "    # we will make some changes to x_control in this function, so make a copy in order to preserve the origianl referenced object\n",
        "    x_control_internal = deepcopy(x_control)\n",
        "\n",
        "    s_attr_to_fp_fn = {}\n",
        "    \n",
        "    s_attr_vals = x_control_internal\n",
        "    for s_val in sorted(list(set(s_attr_vals))):\n",
        "        s_attr_to_fp_fn[s_val] = {}\n",
        "        y_true_local = y_true[s_attr_vals==s_val]\n",
        "        y_pred_local = y_pred[s_attr_vals==s_val]\n",
        "\n",
        "        \n",
        "\n",
        "        acc = float(sum(y_true_local==y_pred_local)) / len(y_true_local)\n",
        "\n",
        "        fp = sum(np.logical_and(y_true_local == -1.0, y_pred_local == +1.0)) # something which is -ve but is misclassified as +ve\n",
        "        fn = sum(np.logical_and(y_true_local == +1.0, y_pred_local == -1.0)) # something which is +ve but is misclassified as -ve\n",
        "        tp = sum(np.logical_and(y_true_local == +1.0, y_pred_local == +1.0)) # something which is +ve AND is correctly classified as +ve\n",
        "        tn = sum(np.logical_and(y_true_local == -1.0, y_pred_local == -1.0)) # something which is -ve AND is correctly classified as -ve\n",
        "\n",
        "        all_neg = sum(y_true_local == -1.0)\n",
        "        all_pos = sum(y_true_local == +1.0)\n",
        "\n",
        "        fpr = float(fp) / float(fp + tn)\n",
        "        fnr = float(fn) / float(fn + tp)\n",
        "        tpr = float(tp) / float(tp + fn)\n",
        "        tnr = float(tn) / float(tn + fp)\n",
        "\n",
        "\n",
        "        s_attr_to_fp_fn[s_val][\"fp\"] = fp\n",
        "        s_attr_to_fp_fn[s_val][\"fn\"] = fn\n",
        "        s_attr_to_fp_fn[s_val][\"fpr\"] = fpr\n",
        "        s_attr_to_fp_fn[s_val][\"fnr\"] = fnr\n",
        "\n",
        "        s_attr_to_fp_fn[s_val][\"acc\"] = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "    return s_attr_to_fp_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "SeCICw-D712R",
      "metadata": {
        "id": "SeCICw-D712R"
      },
      "outputs": [],
      "source": [
        "def get_sensitive_attr_constraint_fpr_fnr_cov(model, x_arr, y_arr_true, y_arr_dist_boundary, x_control_arr, verbose=False):\n",
        "        \n",
        "    assert(x_arr.shape[0] == x_control_arr.shape[0])\n",
        "    if len(x_control_arr.shape) > 1: # make sure we just have one column in the array\n",
        "        assert(x_control_arr.shape[1] == 1)\n",
        "    if len(set(x_control_arr)) != 2: # non binary attr\n",
        "        raise Exception(\"Non binary attr, fix to handle non bin attrs\")\n",
        "\n",
        "    \n",
        "    arr = []\n",
        "    if model is None:\n",
        "        arr = y_arr_dist_boundary * y_arr_true # simply the output labels\n",
        "    else:\n",
        "        arr = np.dot(model, x_arr.T) * y_arr_true # the product with the weight vector -- the sign of this is the output label\n",
        "    arr = np.array(arr)\n",
        "\n",
        "    s_val_to_total = {ct:{} for ct in [0,1,2]}\n",
        "    s_val_to_avg = {ct:{} for ct in [0,1,2]}\n",
        "    cons_sum_dict = {ct:{} for ct in [0,1,2]} # sum of entities (females and males) in constraints are stored here\n",
        "\n",
        "    for v in set(x_control_arr):\n",
        "        s_val_to_total[0][v] = sum(x_control_arr == v)\n",
        "        s_val_to_total[1][v] = sum(np.logical_and(x_control_arr == v, y_arr_true == -1))\n",
        "        s_val_to_total[2][v] = sum(np.logical_and(x_control_arr == v, y_arr_true == +1))\n",
        "\n",
        "\n",
        "    for ct in [0,1,2]:\n",
        "        s_val_to_avg[ct][0] = s_val_to_total[ct][1] / float(s_val_to_total[ct][0] + s_val_to_total[ct][1]) # N1 / N\n",
        "        s_val_to_avg[ct][1] = 1.0 - s_val_to_avg[ct][0] # N0 / N\n",
        "\n",
        "    \n",
        "    for v in set(x_control_arr):\n",
        "        idx = x_control_arr == v\n",
        "        dist_bound_prod = arr[idx]\n",
        "\n",
        "        cons_sum_dict[0][v] = sum( np.minimum(0, dist_bound_prod) ) * (s_val_to_avg[0][v] / len(x_arr))\n",
        "        cons_sum_dict[1][v] = sum( np.minimum(0, ( (1 - y_arr_true[idx]) / 2 ) * dist_bound_prod) ) * (s_val_to_avg[1][v] / sum(y_arr_true == -1))\n",
        "        cons_sum_dict[2][v] = sum( np.minimum(0, ( (1 + y_arr_true[idx]) / 2 ) * dist_bound_prod) ) * (s_val_to_avg[2][v] / sum(y_arr_true == +1))\n",
        "        \n",
        "\n",
        "    cons_type_to_name = {0:\"ALL\", 1:\"FPR\", 2:\"FNR\"}\n",
        "    for cons_type in [0,1,2]:\n",
        "        cov_type_name = cons_type_to_name[cons_type]    \n",
        "        cov = cons_sum_dict[cons_type][1] - cons_sum_dict[cons_type][0]\n",
        "        \n",
        "    return cons_sum_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "kX-R6Az1715J",
      "metadata": {
        "id": "kX-R6Az1715J"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(model, x_train, y_train, x_test, y_test, y_train_predicted, y_test_predicted):\n",
        "    if model is not None and y_test_predicted is not None:\n",
        "        print(\"Either the model (w) or the predicted labels should be None\")\n",
        "        raise Exception(\"Either the model (w) or the predicted labels should be None\")\n",
        "\n",
        "    if model is not None:\n",
        "        y_test_predicted = np.sign(np.dot(x_test, model))\n",
        "        y_train_predicted = np.sign(np.dot(x_train, model))\n",
        "\n",
        "    def get_accuracy(y, Y_predicted):\n",
        "        correct_answers = (Y_predicted == y).astype(int) # will have 1 when the prediction and the actual label match\n",
        "        accuracy = float(sum(correct_answers)) / float(len(correct_answers))\n",
        "        return accuracy, sum(correct_answers)\n",
        "\n",
        "    train_score, correct_answers_train = get_accuracy(y_train, y_train_predicted)\n",
        "    test_score, correct_answers_test = get_accuracy(y_test, y_test_predicted)\n",
        "\n",
        "    return train_score, test_score, correct_answers_train, correct_answers_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fFxZmth37173",
      "metadata": {
        "id": "fFxZmth37173"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    w = train_model(X_train,y_train,x_race_train,eps,constraint_params)\n",
        "    train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = get_clf_stats(w, X_train, y_train, x_race_train, X_test, y_test, x_race_test)\n",
        "    return w, test_score, s_attr_to_fp_fn_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dm579lSlnWsx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "Dm579lSlnWsx",
        "outputId": "36e7d19c-9d83-4738-f103-35752e00f662"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9bb57fb39786>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_all_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_all_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_attr_to_fp_fn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_attr_to_fp_fn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clf_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_race_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_race_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitive_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sensitive_attrs' is not defined"
          ]
        }
      ],
      "source": [
        "train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = get_clf_stats(w, X_train, y_train, x_race_train, X_test, y_test, x_race_test, sensitive_attrs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "h-MBV80KndaS",
      "metadata": {
        "id": "h-MBV80KndaS"
      },
      "outputs": [],
      "source": [
        "eps = 1e-6\n",
        "sensitive_attrs_to_cov_thresh = {0:{0:0, 1:0}, 1:{0:0, 1:0}, 2:{0:0, 1:0}} # zero covariance threshold, means try to get the fairest solution\n",
        "tau = 5.0\n",
        "mu = 1.2\n",
        "idx_to_constraint = {-1:\"No Constraint\",0:\"Misclassification\",1:\"Only FPR\",2:\"Only FNR\",4:\"Both FPR and FNR\"}\n",
        "summary_W = pd.DataFrame(columns=[\"Constraint_type\",\"Accuracy\",\"FPR\",\"FNR\"])\n",
        "summary_B = pd.DataFrame(columns=[\"Constraint_type\",\"Accuracy\",\"FPR\",\"FNR\"])\n",
        "summary_diff = pd.DataFrame(columns=[\"Constraint_type\",\"Overall Accuracy\",\"Diff_Accuracy\",\"Diff_FPR\",\"Diff_FNR\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "hX5gMu-LfQdP",
      "metadata": {
        "id": "hX5gMu-LfQdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0daebc78-69c5-400b-d19c-67a72576d113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 1 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy: 0.6615384615384615\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b655b4ac98e7>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 2 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 3 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 4 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy: 0.6615384615384615\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b655b4ac98e7>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 5 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 6 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 7 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy: 0.6473372781065089\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b655b4ac98e7>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 8 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 9 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 10 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy: 0.6473372781065089\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b655b4ac98e7>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 11 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 12 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/cvxpy/expressions/expression.py:612: UserWarning: \n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 13 times so far.\n",
            "\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Accuracy: 0.6497041420118344\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b655b4ac98e7>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
            "<ipython-input-18-b655b4ac98e7>:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "for i in [-1,0,1,2,4]:\n",
        "    if i == -1:\n",
        "      constraint_params = None\n",
        "    else:\n",
        "      constraint_params = {\"cons_type\": i, \n",
        "              \"tau\": tau, \n",
        "              \"mu\": mu, \n",
        "              \"sensitive_attrs_to_cov_thresh\": sensitive_attrs_to_cov_thresh}\n",
        "    w, test_score, s_attr_to_fp_fn_test = train()\n",
        "    summary_W = summary_W.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[0]['acc'],\"FPR\":s_attr_to_fp_fn_test[0]['fpr'],\"FNR\":s_attr_to_fp_fn_test[0]['fnr']},ignore_index=True)\n",
        "    summary_B = summary_B.append({\"Constraint_type\":idx_to_constraint[i],\"Accuracy\":s_attr_to_fp_fn_test[1]['acc'],\"FPR\":s_attr_to_fp_fn_test[1]['fpr'],\"FNR\":s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)\n",
        "    summary_diff = summary_diff.append({\"Constraint_type\":idx_to_constraint[i],\"Overall Accuracy\":test_score,\"Diff_Accuracy\":s_attr_to_fp_fn_test[0]['acc']-s_attr_to_fp_fn_test[1]['acc'],\"Diff_FPR\":s_attr_to_fp_fn_test[0]['fpr']-s_attr_to_fp_fn_test[1]['fpr'],\"Diff_FNR\":s_attr_to_fp_fn_test[0]['fnr']-s_attr_to_fp_fn_test[1]['fnr']},ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "wO8MukIt1aHl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO8MukIt1aHl",
        "outputId": "b9847c4d-9479-4c48-e3e7-335e13400881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for sensitive attribute value White (Caucasian):\n",
            "     Constraint_type  Accuracy       FPR       FNR\n",
            "0      No Constraint  0.657033  0.368421  0.323024\n",
            "1  Misclassification  0.660886  0.372807  0.312715\n",
            "2           Only FPR  0.630058  0.271930  0.446735\n",
            "3           Only FNR  0.637765  0.333333  0.384880\n",
            "4   Both FPR and FNR  0.641618  0.315789  0.391753\n"
          ]
        }
      ],
      "source": [
        "print(\"Summary for sensitive attribute value White (Caucasian):\")\n",
        "print(summary_W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Kgn9hzPS16tP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgn9hzPS16tP",
        "outputId": "6946a9ef-b21b-4b84-aaf9-6525b981b880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for sensitive attribute value Black (African-American):\n",
            "     Constraint_type  Accuracy       FPR       FNR\n",
            "0      No Constraint  0.668712  0.117318  0.591837\n",
            "1  Misclassification  0.662577  0.162011  0.551020\n",
            "2           Only FPR  0.674847  0.178771  0.503401\n",
            "3           Only FNR  0.662577  0.223464  0.476190\n",
            "4   Both FPR and FNR  0.662577  0.223464  0.476190\n"
          ]
        }
      ],
      "source": [
        "print(\"Summary for sensitive attribute value Black (African-American):\")\n",
        "print(summary_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ft0KvQUp1tml",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft0KvQUp1tml",
        "outputId": "faded322-bb47-4791-9a55-0ed6b0916b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of differences between sensitive attribute values:\n",
            "     Constraint_type  Overall Accuracy  Diff_Accuracy  Diff_FPR  Diff_FNR\n",
            "0      No Constraint          0.661538      -0.011679  0.251103 -0.268813\n",
            "1  Misclassification          0.661538      -0.001690  0.210796 -0.238306\n",
            "2           Only FPR          0.647337      -0.044789  0.093159 -0.056666\n",
            "3           Only FNR          0.647337      -0.024812  0.109870 -0.091311\n",
            "4   Both FPR and FNR          0.649704      -0.020958  0.092326 -0.084438\n"
          ]
        }
      ],
      "source": [
        "print(\"Summary of differences between sensitive attribute values:\")\n",
        "print(summary_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0PF1ns3pxIKS",
      "metadata": {
        "id": "0PF1ns3pxIKS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}