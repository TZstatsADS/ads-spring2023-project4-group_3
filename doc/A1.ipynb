{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A8lU9AzjTxKf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean, std\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvCRecr0TxKg"
   },
   "source": [
    "## Data Edit & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rgJ1qAxCTxKh"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df =pd.read_csv('https://github.com/propublica/compas-analysis/raw/master/compas-scores-two-years.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkoTSdpRTxKk"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gj0UYK7TxKk"
   },
   "source": [
    "Split the dataset into training, validation and testing set (0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tXzDO578PzkJ"
   },
   "outputs": [],
   "source": [
    "# We focus on the sensitive attribute: race\n",
    "# Caucasian: 1; African-American: 0\n",
    "is_race = [i=='African-American' or i=='Caucasian' for i in df.race]\n",
    "df = df.loc[is_race]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W2GyrWmiPzhc"
   },
   "outputs": [],
   "source": [
    "# If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, \n",
    "# we assume that because of data quality reasons, that we do not have the right offense.\n",
    "is_date = [i<=30 and i>=-30 for i in df.days_b_screening_arrest]\n",
    "df = df.loc[is_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IvZ2IixqPzgO"
   },
   "outputs": [],
   "source": [
    "# The recidivist flag -- is_recid -- is encoded as -1 if we could not find a compas case at all.\n",
    "# Remove rows with is_recid=-1\n",
    "df = df.loc[df.is_recid != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fRiyclfnP7ZC"
   },
   "outputs": [],
   "source": [
    "# Those with a c_charge_degree of 'O' -- will not result in Jail time are removed\n",
    "df = df.loc[df.c_charge_degree != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VaTBIaE8P7Wr"
   },
   "outputs": [],
   "source": [
    "# dropna and select required columns\n",
    "col = df.isna().sum().sort_values()[-14:].index.tolist()\n",
    "df = df.drop(columns = col)\n",
    "df = df.dropna()\n",
    "df = df.loc[df.score_text != 'N/A',['sex','race','decile_score','two_year_recid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KtEL0POzQBDH"
   },
   "outputs": [],
   "source": [
    "# Turn 'sex' and 'race' into dummy variables\n",
    "df['sex'].replace(['Male','Female'],[1,0], inplace = True)\n",
    "df['race'].replace(['Caucasian','African-American'],[1,0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5U5TJbwhQBA3"
   },
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "# training : validation : testing = 5:1:1\n",
    "data_train, data_test = train_test_split(df, test_size=0.14, random_state=15)\n",
    "data_train, data_val= train_test_split(data_train, test_size=0.16, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZyUQXJhTxKl"
   },
   "source": [
    "# Learning Fair Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo-GbWOTTxKl"
   },
   "source": [
    "$d(x_n, v_k) = ||x_n − v_k||_2$\n",
    "<br>\n",
    "$d(x_n, v_k, α) = \\sum\\limits _{i=1}^{D} α_i(x_{ni} - v_{ki})^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B2vOiXriTxKl"
   },
   "outputs": [],
   "source": [
    "# distance - d(x_n, v_k, alpha)\n",
    "def d(X, v, alpha):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = len(v)\n",
    "    res = np.zeros((N, K))\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            for d in range(D):\n",
    "                res[n, k] += alpha[d]*(X.iloc[n][d] - v[k, d])**2\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22M6PVCuTxKl"
   },
   "source": [
    "$M_{n,k}=P(Z=k|x_n)=exp(-d(x,v_k))/\\sum\\limits_{j=1}^{k}exp(-d(x,v_j))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BHXPFfhqDnQt"
   },
   "outputs": [],
   "source": [
    "def M_nk(dist, k):\n",
    "    N, K = dist.shape\n",
    "    expo_res = np.exp(-1 * dist)\n",
    "    deno = np.sum(expo_res, axis=1).reshape(-1, 1)\n",
    "    M_nk = expo_res / deno\n",
    "    return M_nk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXacEJI0TxKm"
   },
   "source": [
    "$M_k = \\frac{1}{|X_0|} \\sum_{n \\in X_0} M_{nk}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "SlaTSWv3CW9c"
   },
   "outputs": [],
   "source": [
    "def M_k(X, M_nk, k):\n",
    "    N, K = M_nk.shape\n",
    "    M_k_values = np.sum(M_nk, axis=0) / N\n",
    "    return M_k_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdAw2c2ETxKm"
   },
   "source": [
    "$\n",
    "\\hat{x}_n = \\sum^K_{k=1}M_{nk}v_k\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "L_x = \\sum_{n=1}^N (x_n - \\hat{x}_n)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LqP1DTI3TxKm"
   },
   "outputs": [],
   "source": [
    "#  the reconstruction of x_n  and L_x\n",
    "def xnhat(X, M_nk, v):\n",
    "    N,K = M_nk.shape\n",
    "    D = X.shape[1]\n",
    "    x_n_hat = np.zeros((N, D))\n",
    "    L_x = 0\n",
    "    for n in range(N):\n",
    "        for d in range(D):\n",
    "            for k in range(K):\n",
    "                x_n_hat[n, d] += M_nk[n, k]*v[k, d]\n",
    "        # calculate L_x        \n",
    "        L_x += (X.iloc[n][d] - x_n_hat[n, d])**2   \n",
    "    return x_n_hat, L_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vGUijdDhptq"
   },
   "source": [
    "$\\hat{y_n}=\\sum\\limits_{k = 1}^{K}M_{n,k}w_k$, we constrain the $w_k$ values to be between 0 and 1.\n",
    "\n",
    "$L_y = \\sum_{n=1}^N -y_n log \\hat{y}_n - (1-y_n)log(1- \\hat{y}_n) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fl1rRKUW-o6R"
   },
   "outputs": [],
   "source": [
    "def ynhat(M_nk, w, y):\n",
    "    N, K = M_nk.shape\n",
    "    y_hat = np.dot(M_nk, w)\n",
    "    log_loss_terms = -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n",
    "    L_y = log_loss_terms.sum()\n",
    "    return y_hat, L_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xfL50c-TxKm"
   },
   "source": [
    "We are going to minimize the Object function $L$\n",
    "\n",
    "$L = A_z L_Z + A_x L_x + A_y L_y$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "u6zN4Tkb5_2f"
   },
   "outputs": [],
   "source": [
    "def L(params, sensitive_data, nonsensitive_data, sensitive_labels, nonsensitive_labels, K, A_z, A_x, A_y):\n",
    "    sensitive_samples, sensitive_features = sensitive_data.shape\n",
    "    nonsensitive_samples, nonsensitive_features = nonsensitive_data.shape\n",
    "\n",
    "    alpha_sensitive = params[:sensitive_features]\n",
    "    alpha_nonsensitive = params[sensitive_features : 2 * sensitive_features]\n",
    "    w = params[2 * sensitive_features : (2 * sensitive_features) + K]\n",
    "    v = np.matrix(params[(2 * sensitive_features) + K:]).reshape((K, sensitive_features))\n",
    "\n",
    "    dist_sensitive = d(sensitive_data, v, alpha_sensitive)\n",
    "    dist_nonsensitive = d(nonsensitive_data, v, alpha_nonsensitive)\n",
    "\n",
    "    M_nk_sensitive = M_nk(dist_sensitive, K)\n",
    "    M_nk_nonsensitive = M_nk(dist_nonsensitive, K)\n",
    "\n",
    "    M_k_sensitive = M_k(sensitive_data, M_nk_sensitive, K)\n",
    "    M_k_nonsensitive = M_k(nonsensitive_data, M_nk_nonsensitive, K)\n",
    "\n",
    "    L_z = sum(abs(M_k_sensitive[k] - M_k_nonsensitive[k]) for k in range(K))\n",
    "\n",
    "    x_hat_sensitive, L_x_sensitive = xnhat(sensitive_data, M_nk_sensitive, v)\n",
    "    x_hat_nonsensitive, L_x_nonsensitive = xnhat(nonsensitive_data, M_nk_nonsensitive, v)\n",
    "    L_x = L_x_sensitive + L_x_nonsensitive\n",
    "\n",
    "    y_hat_sensitive, L_y_sensitive = ynhat(M_nk_sensitive, w, sensitive_labels)\n",
    "    y_hat_nonsensitive, L_y_nonsensitive = ynhat(M_nk_nonsensitive, w, nonsensitive_labels)\n",
    "    L_y = L_y_sensitive + L_y_nonsensitive\n",
    "\n",
    "    loss = A_z * L_z + A_x * L_x + A_y * L_y\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F9yyQj_QjPvb"
   },
   "outputs": [],
   "source": [
    "def cal_pred(params, D, K, sen_dt, nsen_dt, sen_label, nsen_label):\n",
    "    # Extract and reshape parameters\n",
    "    alphas_sen = params[:D]\n",
    "    alphas_nsen = params[D : 2 * D]\n",
    "    weights = params[2 * D : (2 * D) + K]\n",
    "    v_matrix = np.matrix(params[(2 * D) + K:]).reshape((K, D))\n",
    "    \n",
    "    # Compute distance matrices for sen and nsen data\n",
    "    dist_sen = d(sen_dt, v_matrix, alphas_sen)\n",
    "    dist_nsen = d(nsen_dt, v_matrix, alphas_nsen) \n",
    "    \n",
    "    # Compute M_nk matrices for sen and nsen data\n",
    "    M_nk_sen = M_nk(dist_sen, K)\n",
    "    M_nk_nsen = M_nk(dist_nsen, K)\n",
    "    \n",
    "    # Compute y_n_hat matrices and likelihoods for sen and nsen data\n",
    "    y_hat_sen, likelihood_sen = ynhat(M_nk_sen, weights, sen_label)\n",
    "    y_hat_nsen, likelihood_nsen = ynhat(M_nk_nsen, weights, nsen_label)\n",
    "    \n",
    "    return y_hat_sen, y_hat_nsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E_yXLEFuTxKn"
   },
   "outputs": [],
   "source": [
    "# calculates the overall accuracy, \n",
    "# accuracy for African-American group\n",
    "# accuracy for Caucasian group\n",
    "# and calibration of the model\n",
    "\n",
    "def cal_calibr(y_pred_sensitive, y_pred_nonsensitive, y_sensitive_labels, y_nonsensitive_labels):\n",
    "\n",
    "    # Convert the predictions using the threshold function\n",
    "    converted_y_pred_sensitive = [1 if pred >= 0.5 else 0 for pred in y_pred_sensitive] \n",
    "    converted_y_pred_nonsensitive = [1 if pred >= 0.5 else 0 for pred in y_pred_nonsensitive] \n",
    "\n",
    "    # Calculate the accuracy for sensitive and nonsensitive groups\n",
    "    accuracy_sensitive = accuracy_score(y_sensitive_labels, converted_y_pred_sensitive)\n",
    "    accuracy_nonsensitive = accuracy_score(y_nonsensitive_labels, converted_y_pred_nonsensitive)\n",
    "\n",
    "    # Combine the labels and predictions for calculating the overall accuracy\n",
    "    combined_labels = y_sensitive_labels.append(y_nonsensitive_labels)\n",
    "    combined_predictions = np.concatenate((converted_y_pred_sensitive, converted_y_pred_nonsensitive), axis=0)\n",
    "    \n",
    "    # Calculate the overall accuracy\n",
    "    overall_accuracy = accuracy_score(combined_labels, combined_predictions)\n",
    "\n",
    "    # Calculate the calibration of the model\n",
    "    calibration = abs(accuracy_sensitive - accuracy_nonsensitive)\n",
    "\n",
    "    print(\"The overall accuracy is: \", overall_accuracy)\n",
    "    print(\"The accuracy for African-American is: \", accuracy_sensitive)\n",
    "    print(\"The accuracy for Caucasian is: \", accuracy_nonsensitive)\n",
    "    print(\"The calibration of the model is: \", calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZJajmGpZsOyS"
   },
   "outputs": [],
   "source": [
    "def LFR(training_data, val_data, y_name, sensitive_variable_name, K, A_z, A_x, A_y):\n",
    "    # Split the training dataset into sensitive and non-sensitive groups\n",
    "    sensitive_training = training_data[training_data[sensitive_variable_name] == 0]\n",
    "    non_sensitive_training = training_data[training_data[sensitive_variable_name] == 1]\n",
    "\n",
    "    # Split the validation dataset\n",
    "    sensitive_validation = val_data[val_data[sensitive_variable_name] == 0]\n",
    "    non_sensitive_validation = val_data[val_data[sensitive_variable_name] == 1]\n",
    "\n",
    "    # Remove sensitive variable\n",
    "    for dataset in [sensitive_training, sensitive_validation, non_sensitive_training, non_sensitive_validation]:\n",
    "        dataset.drop(columns=[sensitive_variable_name], inplace=True)\n",
    "\n",
    "    # Assign target variable (y) and remove it from the datasets\n",
    "    y_sensitive_training = sensitive_training.pop(y_name)\n",
    "    y_sensitive_validation = sensitive_validation.pop(y_name)\n",
    "    y_non_sensitive_training = non_sensitive_training.pop(y_name)\n",
    "    y_non_sensitive_validation = non_sensitive_validation.pop(y_name)\n",
    "\n",
    "    # Generate random values for alpha and w\n",
    "    alpha_sensitive = np.random.dirichlet(np.ones(sensitive_training.shape[1]), size=1).flatten()\n",
    "    alpha_non_sensitive = np.random.dirichlet(np.ones(non_sensitive_training.shape[1]), size=1).flatten()\n",
    "    w = np.random.dirichlet(np.ones(K), size=1).flatten()\n",
    "    v = np.random.random((K, sensitive_training.shape[1]))\n",
    "\n",
    "    initial_parameters = np.concatenate([alpha_sensitive, alpha_non_sensitive, w, v.flatten()])\n",
    "\n",
    "    # Define parameter boundaries\n",
    "    bounds = [(0, 1)] * (sensitive_training.shape[1] + non_sensitive_training.shape[1] + K) + [(None, None)] * (K * sensitive_training.shape[1])\n",
    "\n",
    "    # Minimize the metric\n",
    "    optimal_params, min_L, _ = optim.fmin_l_bfgs_b(L, x0=initial_parameters, epsilon=1e-5,\n",
    "                                                   args=(sensitive_training, non_sensitive_training, y_sensitive_training,\n",
    "                                                         y_non_sensitive_training, K, A_z, A_x, A_y),\n",
    "                                                   bounds=bounds, approx_grad=True,\n",
    "                                                   maxfun=150000, maxiter=150000)\n",
    "\n",
    "    # Predict y_n_hat for the training dataset\n",
    "    y_hat_sensitive_train, y_hat_non_sensitive_train = cal_pred(optimal_params, sensitive_training.shape[1], K, sensitive_training,\n",
    "                                                                 non_sensitive_training, y_sensitive_training, y_non_sensitive_training)\n",
    "\n",
    "    print(\"Training accuracy:\")\n",
    "    cal_calibr(y_hat_sensitive_train, y_hat_non_sensitive_train, y_sensitive_training, y_non_sensitive_training)\n",
    "\n",
    "    # Predict y_n_hat for the validation dataset\n",
    "    y_hat_sensitive_val, y_hat_non_sensitive_val = cal_pred(optimal_params, sensitive_validation.shape[1], K, sensitive_validation,\n",
    "                                                             non_sensitive_validation, y_sensitive_validation, y_non_sensitive_validation)\n",
    "\n",
    "    print(\"Validation accuracy:\")\n",
    "    cal_calibr(y_hat_sensitive_val, y_hat_non_sensitive_val, y_sensitive_validation, y_non_sensitive_validation)\n",
    "\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5VrCXUBuTxKo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:\n",
      "The overall accuracy is:  0.53125\n",
      "The accuracy for African-American is:  0.4842740198190435\n",
      "The accuracy for Caucasian is:  0.6045729657027572\n",
      "The calibration of the model is:  0.1202989458837137\n",
      "Validation accuracy:\n",
      "The overall accuracy is:  0.5165289256198347\n",
      "The accuracy for African-American is:  0.4671201814058957\n",
      "The accuracy for Caucasian is:  0.5929824561403508\n",
      "The calibration of the model is:  0.12586227473445516\n",
      "The training time: 663.7078614234924\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "para_test = LFR(data_train, data_val, 'two_year_recid', 'race', 10, 0.3, 0.3, 0.4)\n",
    "end = time.time() \n",
    "print( f\"The training time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tHgZFCrl2jvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime of testing LFR model: 0.44855690002441406\n",
      "The overall accuracy is:  0.530446549391069\n",
      "The accuracy for African-American is:  0.44282238442822386\n",
      "The accuracy for Caucasian is:  0.6402439024390244\n",
      "The calibration of the model is:  0.19742151801080055\n"
     ]
    }
   ],
   "source": [
    "# Test the LFR model\n",
    "sensitive_test = data_test[data_test['race']==0]\n",
    "nsensitive_test = data_test[data_test['race']==1]\n",
    "\n",
    "sensitive_test=sensitive_test.drop(columns=['race'])\n",
    "nsensitive_test = nsensitive_test.drop(columns=['race'])\n",
    "\n",
    "y_sensitive_test = sensitive_test['two_year_recid']\n",
    "sensitive_test = sensitive_test.drop(columns=['two_year_recid'])\n",
    "\n",
    "y_nsensitive_test = nsensitive_test['two_year_recid']\n",
    "nsensitive_test = nsensitive_test.drop(columns=['two_year_recid'])\n",
    "\n",
    "start = time.time()\n",
    "y_hat_sen_test, y_hat_nsen_test = cal_pred(para_test, sensitive_test.shape[1], 10, sensitive_test, \n",
    "             nsensitive_test, y_sensitive_test, y_nsensitive_test)\n",
    "end = time.time() \n",
    "print( f\"runtime of testing LFR model: {end-start}\")\n",
    "cal_calibr(y_hat_sen_test, y_hat_nsen_test, y_sensitive_test, y_nsensitive_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
